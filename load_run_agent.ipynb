{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the performance of the agents\n",
    "## Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install mlagents==0.22.0 gym_unity==0.22 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from gym import error, spaces\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.registry import UnityEnvRegistry\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import (\n",
    "    EnvironmentParametersChannel,\n",
    ")\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import (\n",
    "    EngineConfigurationChannel,\n",
    ")\n",
    "from gym_unity.envs import UnityToGymWrapper\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from double_dqn.double_dqn_agent import Agent\n",
    "from double_dqn.q_network import QNetwork\n",
    "\n",
    "\n",
    "\n",
    "reset_parameters = EnvironmentParametersChannel()\n",
    "engine_config = EngineConfigurationChannel()\n",
    "\n",
    "environment_filename='/Users/ankit/Desktop/MTP2.0/Executables/3.app'\n",
    "\n",
    "unity_env = UnityEnvironment(\n",
    "        environment_filename,\n",
    "        1,\n",
    "        timeout_wait=30,\n",
    "        side_channels=[reset_parameters, engine_config],\n",
    "        )   \n",
    "\n",
    "env = UnityToGymWrapper(unity_env, flatten_branched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=1, max_t=3000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    \n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    done_count = 0\n",
    "    file=None\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset()\n",
    "        state = env_info \n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, info = env.step(action)        # send the action to the environment\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            print(reward)\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        \n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        file = open('scores.txt', 'a')\n",
    "        file.write(str(i_episode)+','+ str(score)+\",\"+str(eps)+\"\\n\")\n",
    "        file.close()\n",
    "        if i_episode % 20 == 0:\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkPoint.pth')\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMax Score: {:.2f}'.format(i_episode, np.mean(scores_window), np.max(scores_window)))\n",
    "        if np.mean(scores_window)>=40.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint_dueling_ddqn.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=83, action_size=4, seed=0, QNetwork=QNetwork)\n",
    "scores = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(agent):\n",
    "    env_info = env.reset()\n",
    "    state = env_info            \n",
    "    score = 0                                          # initialize the score\n",
    "    for i in range(1000):\n",
    "        action = agent.act(state, 0.2)                      # select an action\n",
    "        print(action)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        score += reward                                # update the score\n",
    "#         print(state)\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "\n",
    "    print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for inferencing the environment.\n",
    "agent = Agent(state_size=83, action_size=4, seed=0, QNetwork=QNetwork)\n",
    "state_dict = torch.load('checkPoint.pth',map_location=torch.device('cpu'))\n",
    "agent.qnetwork_local.load_state_dict(state_dict)\n",
    "run(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
